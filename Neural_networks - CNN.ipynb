{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vH9yjLZspIuO"
   },
   "source": [
    "#### 1. Introducción\n",
    "\n",
    "El presente trabajo busca analizar desde la perspectiva de las redes neuronales convolucionales el dataset conformado por 1125 imágenes relativos al clima. El mismo fue descargado del repositorio 'Mendeley Data' (url: https://data.mendeley.com/datasets/4drtyfjtfy/1). El dataset  contiene imágenes de cielos nublados, lluviosos, soleados, y de amaneceres.\n",
    "El objetivo es aplicar las redes neuronales, de manera de predecir el tipo de cielo de cada imagen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcQrAzglqKM3"
   },
   "source": [
    "El primer paso ha sido crear carpetas de 'train' y 'test' para cada tipo de cielo, con una proporción 0.8 y 0.2 respectivamente. Luego se han cargado dichas carpetas a google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ts__gkBL408P",
    "outputId": "62e2c53c-e86f-4197-ebc3-0e632d1f428b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Monto la unidad del drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNODpD0atON5"
   },
   "source": [
    "#### 2. Convolución "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HN9-SgwCZAGY"
   },
   "source": [
    "Carga de librerías e inicialización de la CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MrprlcvztNL0"
   },
   "outputs": [],
   "source": [
    "# Carga de librerías necesarias\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "c5HPj3OXuJ4e"
   },
   "outputs": [],
   "source": [
    "# Inicialización de la CNN (no requiere la introducción de parámetros)\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmN0A0eRuoSD"
   },
   "source": [
    "A partir de la imagen de entrada y utlizando diversos detectores de rasgos, se procede a la creación de los mapas de características que conforman la capa de convolución.\n",
    "Tal como se puede ver debajo, se han especificado 32 detectores de rasgos, de tamaño 3x3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gru-_X3gu7lQ"
   },
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), \n",
    "                      input_shape = (64, 64, 3), activation = \"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFCMTU7MxyM4"
   },
   "source": [
    "A continuación se aplica un max pooling, de manera de reducir la complejidad y permitir a la red neuronal no solo aprender posiciones y colores, sino también transformaciones y escalados de imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "D8JiEJU3x3qc"
   },
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size = (2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iK9_E_idzuYJ"
   },
   "source": [
    "Se añade una segunda capa de convolución y max pooling de iguales características a la ya añadida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5YoqRZO3zwGg"
   },
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), activation = \"relu\"))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3qzY-vBYgMH"
   },
   "source": [
    "Se añade una tercera capa con 64 detectores de rasgos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ppx6O44TYgMI"
   },
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(filters = 64,kernel_size = (3, 3), activation = \"relu\"))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAv6qUDgyd6J"
   },
   "source": [
    "Se sigue con el aplanado de los datos (flattening), pasando de las capas pooling a un vector que se lleve toda la información en una sola dimensión vertical que servirá de capa de entrada de la RNA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bAGY94_Xy0gI"
   },
   "outputs": [],
   "source": [
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6bn6Dgd0Wpe"
   },
   "source": [
    "Se concluye esta sección realizando la full conection, detallando una función de activación de la capa oculta de tipo 'rectificador lineal unitario'. De igual modo, se utiliza para la capa de salida una función 'softmax'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cOSDMpxB0qe4"
   },
   "outputs": [],
   "source": [
    "classifier.add(Dense(units = 64, activation = \"relu\"))\n",
    "classifier.add(Dense(units = 4, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_ZOdGD22HH3"
   },
   "source": [
    "Compilación de la CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XW2i_hCa2MGJ"
   },
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvHmV1qf205E"
   },
   "source": [
    "#### 3. Ajustar la CNN a las imágenes a entrenar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOPNev0k3Za5"
   },
   "source": [
    "Se procede a cargar las imágenes mediante la técnica de aumento de imagen, de modo de evitar el sobreajuste. \n",
    "Mediante esta técnica se realizan transformaciones aleatorias a las imagenes, lo que permite 'multiplicar' artificialmente la cantidad de estas utilizada para entrenar la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dEmI-IWo3A-1",
    "outputId": "f534ae77-e653-4761-f9b1-22e4a05f0e48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 900 images belonging to 4 classes.\n",
      "Found 225 images belonging to 4 classes.\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 280s 10s/step - loss: 0.8960 - accuracy: 0.6167 - val_loss: 0.7718 - val_accuracy: 0.6667\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 11s 376ms/step - loss: 0.6616 - accuracy: 0.7389 - val_loss: 0.5727 - val_accuracy: 0.8578\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 11s 385ms/step - loss: 0.5431 - accuracy: 0.7956 - val_loss: 0.4483 - val_accuracy: 0.8444\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 11s 375ms/step - loss: 0.5729 - accuracy: 0.7711 - val_loss: 0.4645 - val_accuracy: 0.9156\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 11s 377ms/step - loss: 0.4527 - accuracy: 0.8300 - val_loss: 0.3925 - val_accuracy: 0.9156\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 11s 387ms/step - loss: 0.4065 - accuracy: 0.8378 - val_loss: 0.4134 - val_accuracy: 0.8844\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 11s 386ms/step - loss: 0.3658 - accuracy: 0.8500 - val_loss: 0.4161 - val_accuracy: 0.8667\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 11s 377ms/step - loss: 0.3421 - accuracy: 0.8778 - val_loss: 0.3628 - val_accuracy: 0.9244\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 11s 380ms/step - loss: 0.3700 - accuracy: 0.8589 - val_loss: 0.3968 - val_accuracy: 0.8800\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 11s 373ms/step - loss: 0.4223 - accuracy: 0.8333 - val_loss: 0.5142 - val_accuracy: 0.8489\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 11s 392ms/step - loss: 0.3044 - accuracy: 0.8900 - val_loss: 0.3348 - val_accuracy: 0.8978\n",
      "Epoch 12/50\n",
      "29/29 [==============================] - 11s 381ms/step - loss: 0.2979 - accuracy: 0.8878 - val_loss: 0.4101 - val_accuracy: 0.8622\n",
      "Epoch 13/50\n",
      "29/29 [==============================] - 11s 382ms/step - loss: 0.2674 - accuracy: 0.8900 - val_loss: 0.4530 - val_accuracy: 0.8800\n",
      "Epoch 14/50\n",
      "29/29 [==============================] - 11s 382ms/step - loss: 0.2840 - accuracy: 0.8978 - val_loss: 0.3731 - val_accuracy: 0.8933\n",
      "Epoch 15/50\n",
      "29/29 [==============================] - 11s 384ms/step - loss: 0.2332 - accuracy: 0.9200 - val_loss: 0.4967 - val_accuracy: 0.8889\n",
      "Epoch 16/50\n",
      "29/29 [==============================] - 11s 377ms/step - loss: 0.2269 - accuracy: 0.9100 - val_loss: 0.3662 - val_accuracy: 0.9200\n",
      "Epoch 17/50\n",
      "29/29 [==============================] - 11s 390ms/step - loss: 0.2162 - accuracy: 0.9189 - val_loss: 0.3650 - val_accuracy: 0.9378\n",
      "Epoch 18/50\n",
      "29/29 [==============================] - 11s 384ms/step - loss: 0.2501 - accuracy: 0.9111 - val_loss: 0.3503 - val_accuracy: 0.9378\n",
      "Epoch 19/50\n",
      "29/29 [==============================] - 11s 393ms/step - loss: 0.2730 - accuracy: 0.8944 - val_loss: 0.4245 - val_accuracy: 0.8889\n",
      "Epoch 20/50\n",
      "29/29 [==============================] - 11s 386ms/step - loss: 0.2143 - accuracy: 0.9289 - val_loss: 0.7074 - val_accuracy: 0.8356\n",
      "Epoch 21/50\n",
      "29/29 [==============================] - 11s 376ms/step - loss: 0.2120 - accuracy: 0.9222 - val_loss: 0.3559 - val_accuracy: 0.8978\n",
      "Epoch 22/50\n",
      "29/29 [==============================] - 11s 379ms/step - loss: 0.1599 - accuracy: 0.9400 - val_loss: 0.4766 - val_accuracy: 0.8622\n",
      "Epoch 23/50\n",
      "29/29 [==============================] - 11s 376ms/step - loss: 0.2328 - accuracy: 0.9033 - val_loss: 0.3970 - val_accuracy: 0.9067\n",
      "Epoch 24/50\n",
      "29/29 [==============================] - 11s 387ms/step - loss: 0.1717 - accuracy: 0.9400 - val_loss: 0.4528 - val_accuracy: 0.8844\n",
      "Epoch 25/50\n",
      "29/29 [==============================] - 11s 374ms/step - loss: 0.1582 - accuracy: 0.9456 - val_loss: 0.6125 - val_accuracy: 0.8533\n",
      "Epoch 26/50\n",
      "29/29 [==============================] - 11s 379ms/step - loss: 0.1517 - accuracy: 0.9456 - val_loss: 0.3931 - val_accuracy: 0.9333\n",
      "Epoch 27/50\n",
      "29/29 [==============================] - 11s 378ms/step - loss: 0.1685 - accuracy: 0.9278 - val_loss: 0.5147 - val_accuracy: 0.8844\n",
      "Epoch 28/50\n",
      "29/29 [==============================] - 11s 377ms/step - loss: 0.1443 - accuracy: 0.9456 - val_loss: 0.5368 - val_accuracy: 0.8756\n",
      "Epoch 29/50\n",
      "29/29 [==============================] - 11s 378ms/step - loss: 0.1082 - accuracy: 0.9544 - val_loss: 0.3773 - val_accuracy: 0.9333\n",
      "Epoch 30/50\n",
      "29/29 [==============================] - 11s 382ms/step - loss: 0.1677 - accuracy: 0.9456 - val_loss: 0.4969 - val_accuracy: 0.8933\n",
      "Epoch 31/50\n",
      "29/29 [==============================] - 11s 381ms/step - loss: 0.1727 - accuracy: 0.9267 - val_loss: 0.3268 - val_accuracy: 0.9244\n",
      "Epoch 32/50\n",
      "29/29 [==============================] - 11s 386ms/step - loss: 0.2105 - accuracy: 0.9178 - val_loss: 0.4772 - val_accuracy: 0.9200\n",
      "Epoch 33/50\n",
      "29/29 [==============================] - 11s 383ms/step - loss: 0.1261 - accuracy: 0.9567 - val_loss: 0.4593 - val_accuracy: 0.9200\n",
      "Epoch 34/50\n",
      "29/29 [==============================] - 11s 385ms/step - loss: 0.1016 - accuracy: 0.9678 - val_loss: 0.5511 - val_accuracy: 0.8889\n",
      "Epoch 35/50\n",
      "29/29 [==============================] - 11s 387ms/step - loss: 0.1082 - accuracy: 0.9600 - val_loss: 0.7225 - val_accuracy: 0.8533\n",
      "Epoch 36/50\n",
      "29/29 [==============================] - 11s 382ms/step - loss: 0.1483 - accuracy: 0.9489 - val_loss: 0.4242 - val_accuracy: 0.9378\n",
      "Epoch 37/50\n",
      "29/29 [==============================] - 11s 404ms/step - loss: 0.1065 - accuracy: 0.9622 - val_loss: 0.4173 - val_accuracy: 0.9111\n",
      "Epoch 38/50\n",
      "29/29 [==============================] - 11s 392ms/step - loss: 0.1341 - accuracy: 0.9500 - val_loss: 0.3205 - val_accuracy: 0.9467\n",
      "Epoch 39/50\n",
      "29/29 [==============================] - 11s 382ms/step - loss: 0.1131 - accuracy: 0.9511 - val_loss: 0.5112 - val_accuracy: 0.8889\n",
      "Epoch 40/50\n",
      "29/29 [==============================] - 11s 379ms/step - loss: 0.0820 - accuracy: 0.9678 - val_loss: 0.4319 - val_accuracy: 0.9244\n",
      "Epoch 41/50\n",
      "29/29 [==============================] - 11s 383ms/step - loss: 0.0794 - accuracy: 0.9756 - val_loss: 0.5376 - val_accuracy: 0.9022\n",
      "Epoch 42/50\n",
      "29/29 [==============================] - 11s 385ms/step - loss: 0.0856 - accuracy: 0.9689 - val_loss: 0.5048 - val_accuracy: 0.9156\n",
      "Epoch 43/50\n",
      "29/29 [==============================] - 11s 375ms/step - loss: 0.0875 - accuracy: 0.9689 - val_loss: 0.4794 - val_accuracy: 0.8978\n",
      "Epoch 44/50\n",
      "29/29 [==============================] - 11s 381ms/step - loss: 0.0900 - accuracy: 0.9689 - val_loss: 0.4733 - val_accuracy: 0.9022\n",
      "Epoch 45/50\n",
      "29/29 [==============================] - 11s 383ms/step - loss: 0.1110 - accuracy: 0.9567 - val_loss: 0.6713 - val_accuracy: 0.8667\n",
      "Epoch 46/50\n",
      "29/29 [==============================] - 11s 385ms/step - loss: 0.1120 - accuracy: 0.9644 - val_loss: 0.4861 - val_accuracy: 0.9111\n",
      "Epoch 47/50\n",
      "29/29 [==============================] - 11s 382ms/step - loss: 0.0769 - accuracy: 0.9733 - val_loss: 0.4133 - val_accuracy: 0.9467\n",
      "Epoch 48/50\n",
      "29/29 [==============================] - 11s 376ms/step - loss: 0.0766 - accuracy: 0.9722 - val_loss: 0.5984 - val_accuracy: 0.8978\n",
      "Epoch 49/50\n",
      "29/29 [==============================] - 11s 383ms/step - loss: 0.0623 - accuracy: 0.9789 - val_loss: 0.5372 - val_accuracy: 0.9022\n",
      "Epoch 50/50\n",
      "29/29 [==============================] - 11s 385ms/step - loss: 0.0518 - accuracy: 0.9811 - val_loss: 0.3815 - val_accuracy: 0.9422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f00273ef6d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.3,\n",
    "        zoom_range=0.3,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_dataset = train_datagen.flow_from_directory('/content/drive/MyDrive/Colab Notebooks/dataset/Train',\n",
    "                                                    target_size=(64, 64),\n",
    "                                                    batch_size=32,\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "testing_dataset = test_datagen.flow_from_directory('/content/drive/MyDrive/Colab Notebooks/dataset/Test',\n",
    "                                                target_size=(64, 64),\n",
    "                                                batch_size=32,\n",
    "                                                class_mode='categorical')\n",
    "\n",
    "classifier.fit(training_dataset,\n",
    "                        steps_per_epoch= len(training_dataset),\n",
    "                        epochs=50,\n",
    "                        validation_data=testing_dataset,\n",
    "                        validation_steps=len(testing_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiamXojfYgML"
   },
   "source": [
    "Se observa un excelente comportamiento de la red neuronal, la cual ha obtenido un accuracy de 0.9811 en el conjunto de entrenamiento y un 0.9422 en el de validación."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Trabajo_redes_neuronales_de_convolucion_Scuderi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
